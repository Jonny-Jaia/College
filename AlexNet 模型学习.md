



# AlexNet 模型学习

## AlexNet的主要改进特点

- 使用[^***Relu***]作为激活函数，解决了sigmoid激活函数在网络较深时产生的**梯度弥散**现象。

  [^***Relu***]:Relu激活函数相比于其他的激活函数有更快的收敛速度！

- **Dropout**随机忽略一部分神经元，**避免过拟合**。

- **CNN**使用重叠的**最大池化**

- 提出LRN层--》对局部神经元的活动创建了[^***竞争机制***]，从而提高了网络的**泛化能力**

  [^***竞争机制***]: 响应大对应的值更大，相对的抑制其他反馈较小的神经元.

- CUDA加速深度卷积神经网络的训练，利用GPU的**并行计算能力**，处理训练时大量的矩阵运算

- **数据增强**，随机的从256 * 256的原始图像中截取224  * 224大小的区域（以及水平翻转的镜像），这相当于增加了（256-224）^2=2048倍的数据量。

***

## AlexNet网络的原理

> **整体**：
>
> 包括 **8**个需要训练参数的层（**不包含LRN和池化层**），前五层为卷积层，后三层为全连接层，如图所示。
>
> PS:
>
> + Alexnet最后一层：由1000个类别输出的softmax层。
> + LRN层：出现在**第一个以及第二个卷积层后**。
> + **max-pooling**层：出现在两个LRN层以及最后一个卷积层后
> + Relu激活函数应用在每一层的后面



![img](http://img.blog.csdn.net/20180226220518239) 																			**图一**

![img](https://pic1.zhimg.com/80/v2-c67ca38670e2f14bedec809d8cfc73ec_hd.png)

​																			**图二**

​													**(caffe中的AlexNet网络结构)**

***

> ***超参数***
>
> **input_images:**  shape(224 ,224，3)
>
> **conv1**: 尺寸较大的 卷积核：
>
> + kernel(11,11)
> + stride=4
> + number:96 
>
> **conv1**+**LRN**+**max-pooling**
>
> **max-pooling**:
>
> + kernel(3,3)
> + stride=2
>
> 往后的卷积核尺寸较小，基本都是(5,5),(3,3)大小的，步长都为1；
>
> max-pooling层参数保持不变

***

![img](http://img.blog.csdn.net/20180226220518239)

***逐层分析：***

+ 第一层：

![img](https://img-blog.csdn.net/20180518203413676)

输入原始数据277 * 277 * 3 图像[^图中数据说明]

[^图中数据说明]:最开始是224 * 224 * 3，为后续处理方便必须进行调整

卷积核 11 * 11 * 3,步长 4,个数 96

生成的新像素[^卷积操作大小]：（227-11）/4+1=55

[^卷积操作大小]:[(raw.width-kernel.width)/stride+1,(raw.height-kernel.height)/stride+1]

***卷积输出***

由于卷积核的数量有96个，生成的像素层为55 *55 * 96

采用双GPU并行训练：55 *55 * 96分成两组55 *55 * 48



**重叠pooling层**

最大池化操作，大小3 * 3，步长2

处理后尺寸：（55-3）/2+1=27，27 * 27 *96

**局部响应归一化（LRN）**

 

+ 第二层



****

***特点总结***

+ 前几个卷积层中的虽然计算量较大，但是其中设计的参数量很小，只占有网络总参数的一小部分，这也体现了**卷积层能够通过较小的参数量提取有效的特征**。而相比之下，倘若前几层采用FCN，参数量和计算量均会大的惊人。

  **卷积层对于整个网络的分类能力有着大幅度的提升**

+ 上图中卷积部分都是分**上下两部分**，即这一层计算出来的**featuremap**分开，其中每一层与前一层有虚线连接的数据才流通。

+ **LRN层**：对当前层的输出结果做平滑处理

![img](http://img.blog.csdn.net/20180226220526993)

+ **LRN层：**平滑约束，计算方法

  

![img](http://img.blog.csdn.net/20180226220642522)

***

## AlexNet 基于TensorFlow框架实现



## 基于AlexNet模型的迁移学习

